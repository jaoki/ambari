<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->

<configuration>

  <!-- The properties that end in tar_source describe the pattern of where the tar.gz files come from.
  They will replace {{ hdp_stack_version }} with the "#.#.#.#" value followed by -* (which is the build number in HDP 2.2),
  and treat {{ component_version }} as a wildcard.
  When copying those tarballs, Ambari will look up the corresponding tar_destination_folder property to know where it
  should be copied to.
  All of the destination folders must begin with hdfs://
  Please note that the spaces inside of {{ ... }} are important.
  -->
  <!-- Tez tarball is needed by Hive Server when using the Tez execution egine. -->
  <property>
    <name>tez_tar_source</name>
    <value>/usr/hdp/current/tez-client/lib/tez-{{ component_version }}.{{ hdp_stack_version }}.tar.gz</value>
    <description>Source file path that uses dynamic variables and regex to copy the file to HDFS.</description>
  </property>
  <property>
    <name>tez_tar_destination_folder</name>
    <value>hdfs:///hdp/apps/{{ hdp_stack_version }}/tez/</value>
    <description>Destination HDFS folder for the file.</description>
  </property>

  <!-- Hive tarball is needed by WebHCat. -->
  <property>
    <name>hive_tar_source</name>
    <value>/usr/hdp/current/hive-client/hive-{{ component_version }}.{{ hdp_stack_version }}.tar.gz</value>
    <description>Source file path that uses dynamic variables and regex to copy the file to HDFS.</description>
  </property>
  <property>
    <name>hive_tar_destination_folder</name>
    <value>hdfs:///hdp/apps/{{ hdp_stack_version }}/hive/</value>
    <description>Destination HDFS folder for the file.</description>
  </property>

  <!-- Pig tarball is needed by WebHCat. -->
  <property>
    <name>pig_tar_source</name>
    <value>/usr/hdp/current/pig-client/pig-{{ component_version }}.{{ hdp_stack_version }}.tar.gz</value>
    <description>Source file path that uses dynamic variables and regex to copy the file to HDFS.</description>
  </property>
  <property>
    <name>pig_tar_destination_folder</name>
    <value>hdfs:///hdp/apps/{{ hdp_stack_version }}/pig/</value>
    <description>Destination HDFS folder for the file.</description>
  </property>

  <!-- Hadoop Streaming jar is needed by WebHCat. -->
  <property>
    <name>hadoop-streaming_tar_source</name>
    <value>/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming-{{ component_version }}.{{ hdp_stack_version }}.jar</value>
    <description>Source file path that uses dynamic variables and regex to copy the file to HDFS.</description>
  </property>
  <property>
    <name>hadoop-streaming_tar_destination_folder</name>
    <value>hdfs:///hdp/apps/{{ hdp_stack_version }}/mr/</value>
    <description>Destination HDFS folder for the file.</description>
  </property>

  <!-- Sqoop tarball is needed by WebHCat. -->
  <property>
    <name>sqoop_tar_source</name>
    <value>/usr/hdp/current/sqoop-client/sqoop-{{ component_version }}.{{ hdp_stack_version }}.tar.gz</value>
    <description>Source file path that uses dynamic variables and regex to copy the file to HDFS.</description>
  </property>
  <property>
    <name>sqoop_tar_destination_folder</name>
    <value>hdfs:///hdp/apps/{{ hdp_stack_version }}/sqoop/</value>
    <description>Destination HDFS folder for the file.</description>
  </property>

</configuration>
